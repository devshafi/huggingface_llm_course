{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c26c3cff",
   "metadata": {},
   "source": [
    "### 01 Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e98c8b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain_community -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb73ad2b",
   "metadata": {},
   "source": [
    "### Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8803affb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "629e51ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca13173",
   "metadata": {},
   "source": [
    "# Loading llm using langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329d8a68",
   "metadata": {},
   "source": [
    "## openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62fb36ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm =  OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1cc67e",
   "metadata": {},
   "source": [
    "Various inference techniques are available in langchain. The most common ones are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffa11028",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"What would be good research organization names that are short, catchy, and memorable?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "919c0272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. QuestLab\n",
      "2. InnoScope\n",
      "3. InsightWorks\n",
      "4. NexusResearch\n",
      "5. BrightMind\n",
      "6. NextGenAnalytics\n",
      "7. SparkLab\n",
      "8. StreamlineResearch\n",
      "9. ProdigyStudies\n",
      "10. ThinkTankX\n",
      "11. ImpactResearchCo.\n",
      "12. VisionLab\n",
      "13. ExploreCo.\n",
      "14. CatalystInsights\n",
      "15. BrainwaveR&D\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(llm.predict(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe650fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1671231/1977370505.py:1: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(llm(text))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. ThinkTank\n",
      "2. InsightLab\n",
      "3. Brainwave\n",
      "4. IdeaForge\n",
      "5. MindScope\n",
      "6. QuestGroup\n",
      "7. CuriousMind\n",
      "8. KnowledgeWorks\n",
      "9. ResearchHub\n",
      "10. DiscoveryLab\n"
     ]
    }
   ],
   "source": [
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "624b5993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. InsightLab\n",
      "2. QuestWorks\n",
      "3. MindScope\n",
      "4. DataSphere\n",
      "5. BrightSight\n",
      "6. InnovateLab\n",
      "7. CatalystResearch\n",
      "8. NexusAnalytica\n",
      "9. StriveResearch\n",
      "10. ThinkTank Co.\n",
      "11. ProvenR&D\n",
      "12. ExploreCo.\n",
      "13. VisionScope\n",
      "14. ImpactIntel\n",
      "15. DiscoveryWorks.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21203be",
   "metadata": {},
   "source": [
    "Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fefd1cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. TechGenius\n",
      "2. CodeCrafters\n",
      "3. DigitalSolutions\n",
      "4. ByteBuilders\n",
      "5. CyberPath\n",
      "6. InnovateTech\n",
      "7. FutureVision\n",
      "8. NexTech\n",
      "9. TechHive\n",
      "10. PrimeTech \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(temperature=0.9)\n",
    "name =  llm.predict(\"I want to start a IT company. What would be a good name?\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4f0c5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "It ultimately depends on your target market, industry focus, and brand image. Here are a few suggestions that could work:\n",
      "\n",
      "1. NexusTech Solutions\n",
      "2. InnovateIT\n",
      "3. CodeCrafters\n",
      "4. ByteBuilders\n",
      "5. FutureForge\n",
      "6. DigitalWise\n",
      "7. TechWorks\n",
      "8. NetGenius\n",
      "9. PrimeTech Solutions\n",
      "10. NextLevel IT\n",
      "11. PixelPro Technologies\n",
      "12. CloudLogic\n",
      "13. Synapse Systems\n",
      "14. Prodigy Technologies\n",
      "15. QuantumLeap IT Solutions.\n"
     ]
    }
   ],
   "source": [
    "response = llm(\"I want to start a IT company. What would be a good name?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c98f9e1",
   "metadata": {},
   "source": [
    "## Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90225cd",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05f21d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub  langchain-huggingface ipywidgets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8dcdf02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c47d0e7f7264ef0ada35cc523ee942f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login() # You will be prompted for your HF key, which will then be saved locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "252ffa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf7ed772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c11d13938824c5ab08a2f9431761af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920328b0160a455880afe6c869a1bc57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df3df089e6e4bbf8bddad5e7daece8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6aa1aa43e04ccfa09993201554cdae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f098a17b6cf4a64b7fad8144cb43edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.03,\n",
    ")\n",
    "\n",
    "chat = ChatHuggingFace(llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75f75403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore programmer\", additional_kwargs={}, response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=6, prompt_tokens=22, total_tokens=28), 'model': '', 'finish_reason': 'stop'}, id='run-d69750e4-e45e-443f-ba45-23147f99d547-0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are a helpful translator. Translate the user sentence to French\"),\n",
    "    (\"human\", \"I love programming\"),\n",
    "]\n",
    "\n",
    "response = chat.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38775c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"J'adore programmer\" additional_kwargs={} response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=6, prompt_tokens=22, total_tokens=28), 'model': '', 'finish_reason': 'stop'} id='run-63f54a32-df12-4000-bfeb-0d9a51537700-0'\n"
     ]
    }
   ],
   "source": [
    "for chunk in chat.stream(messages):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5de65f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/Works/Practice/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore programmer.\", additional_kwargs={}, response_metadata={}, id='run-733959fb-5a37-47eb-8bf8-d88b4e1450d9-0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chat.ainvoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d7314174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'tool_calls': [ChatCompletionOutputToolCall(function=ChatCompletionOutputFunctionDefinition(arguments='{\"location\":\"Bangladesh\"}', name='GetPopulation', description=None), id='0', type='function')]} response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=26, prompt_tokens=262, total_tokens=288), 'model': '', 'finish_reason': 'stop'} id='run-4e2430e5-13c9-43b7-ab25-e98b91a046fe-0' tool_calls=[{'name': 'GetPopulation', 'args': {'location': 'Bangladesh'}, 'id': '0', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class GetWeather(BaseModel):\n",
    "    '''Get the current weather in a given location'''\n",
    "\n",
    "    location: str = Field(..., description=\"The city and state,e.g. San Francisco, CA\")\n",
    "\n",
    "class GetPopulation(BaseModel):\n",
    "    '''Get the current population in a given location'''\n",
    "\n",
    "    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
    "\n",
    "chat_with_tools = chat.bind_tools([GetWeather, GetPopulation])\n",
    "ai_msg = chat_with_tools.invoke(\"What is the total population in Bangladesh?\")\n",
    "print(ai_msg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
