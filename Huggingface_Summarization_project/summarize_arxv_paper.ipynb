{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1397654",
   "metadata": {},
   "source": [
    "Install required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01562439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv\n",
      "  Using cached arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting feedparser~=6.0.10 (from arxiv)\n",
      "  Using cached feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: requests~=2.32.0 in /home/jovyan/Works/Practice/.venv/lib/python3.12/site-packages (from arxiv) (2.32.3)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
      "  Using cached sgmllib3k-1.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jovyan/Works/Practice/.venv/lib/python3.12/site-packages (from requests~=2.32.0->arxiv) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jovyan/Works/Practice/.venv/lib/python3.12/site-packages (from requests~=2.32.0->arxiv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jovyan/Works/Practice/.venv/lib/python3.12/site-packages (from requests~=2.32.0->arxiv) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jovyan/Works/Practice/.venv/lib/python3.12/site-packages (from requests~=2.32.0->arxiv) (2025.4.26)\n",
      "Using cached arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
      "Using cached feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [arxiv]\n",
      "\u001b[1A\u001b[2KSuccessfully installed arxiv-2.2.0 feedparser-6.0.11 sgmllib3k-1.0.0\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/jovyan/Works/Practice/.venv/lib/python3.12/site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jovyan/Works/Practice/.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/jovyan/Works/Practice/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [pandas]2m2/3\u001b[0m [pandas]\n",
      "\u001b[1A\u001b[2KSuccessfully installed pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "!pip install arxiv\n",
    "!pip install pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76c49610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import arxiv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6006096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query for fetching papers related to \"deep learning\"\n",
    "query = 'deep learning OR \"deep neural networks\" OR \"deep reinforcement learning\"'\n",
    "search = arxiv.Search(\n",
    "    query=query,\n",
    "    max_results=5,\n",
    "    sort_by=arxiv.SortCriterion.Relevance,\n",
    "    sort_order=arxiv.SortOrder.Descending,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41e3b2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the results in a list\n",
    "papers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6edfe48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_161500/3585603717.py:1: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  results = search.results()\n"
     ]
    }
   ],
   "source": [
    "results = search.results()\n",
    "\n",
    "for result in results:\n",
    "    papers.append({\n",
    "        'title': result.title,\n",
    "        'abstract': result.summary\n",
    "        }        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25193638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep reinforcement learning augments the reinforcement learning framework and\n",
      "utilizes the powerful representation of deep neural networks. Recent works have\n",
      "demonstrated the remarkable successes of deep reinforcement learning in various\n",
      "domains including finance, medicine, healthcare, video games, robotics, and\n",
      "computer vision. In this work, we provide a detailed review of recent and\n",
      "state-of-the-art research advances of deep reinforcement learning in computer\n",
      "vision. We start with comprehending the theories of deep learning,\n",
      "reinforcement learning, and deep reinforcement learning. We then propose a\n",
      "categorization of deep reinforcement learning methodologies and discuss their\n",
      "advantages and limitations. In particular, we divide deep reinforcement\n",
      "learning into seven main categories according to their applications in computer\n",
      "vision, i.e. (i)landmark localization (ii) object detection; (iii) object\n",
      "tracking; (iv) registration on both 2D image and 3D image volumetric data (v)\n",
      "image segmentation; (vi) videos analysis; and (vii) other applications. Each of\n",
      "these categories is further analyzed with reinforcement learning techniques,\n",
      "network design, and performance. Moreover, we provide a comprehensive analysis\n",
      "of the existing publicly available datasets and examine source code\n",
      "availability. Finally, we present some open issues and discuss future research\n",
      "directions on deep reinforcement learning in computer vision\n"
     ]
    }
   ],
   "source": [
    "print(papers[0]['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06728cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the list of dictionaries to a pandas DataFrame\n",
    "df = pd.DataFrame(papers)\n",
    "pd.set_option('display.max_colwidth', None)  # to display full content of the abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc6165a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey</td>\n",
       "      <td>Deep reinforcement learning augments the reinforcement learning framework and\\nutilizes the powerful representation of deep neural networks. Recent works have\\ndemonstrated the remarkable successes of deep reinforcement learning in various\\ndomains including finance, medicine, healthcare, video games, robotics, and\\ncomputer vision. In this work, we provide a detailed review of recent and\\nstate-of-the-art research advances of deep reinforcement learning in computer\\nvision. We start with comprehending the theories of deep learning,\\nreinforcement learning, and deep reinforcement learning. We then propose a\\ncategorization of deep reinforcement learning methodologies and discuss their\\nadvantages and limitations. In particular, we divide deep reinforcement\\nlearning into seven main categories according to their applications in computer\\nvision, i.e. (i)landmark localization (ii) object detection; (iii) object\\ntracking; (iv) registration on both 2D image and 3D image volumetric data (v)\\nimage segmentation; (vi) videos analysis; and (vii) other applications. Each of\\nthese categories is further analyzed with reinforcement learning techniques,\\nnetwork design, and performance. Moreover, we provide a comprehensive analysis\\nof the existing publicly available datasets and examine source code\\navailability. Finally, we present some open issues and discuss future research\\ndirections on deep reinforcement learning in computer vision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Survey Analyzing Generalization in Deep Reinforcement Learning</td>\n",
       "      <td>Reinforcement learning research obtained significant success and attention\\nwith the utilization of deep neural networks to solve problems in high\\ndimensional state or action spaces. While deep reinforcement learning policies\\nare currently being deployed in many different fields from medical applications\\nto large language models, there are still ongoing questions the field is trying\\nto answer on the generalization capabilities of deep reinforcement learning\\npolicies. In this paper, we will formalize and analyze generalization in deep\\nreinforcement learning. We will explain the fundamental reasons why deep\\nreinforcement learning policies encounter overfitting problems that limit their\\ngeneralization capabilities. Furthermore, we will categorize and explain the\\nmanifold solution approaches to increase generalization, and overcome\\noverfitting in deep reinforcement learning policies. From exploration to\\nadversarial analysis and from regularization to robustness our paper provides\\nan analysis on a wide range of subfields within deep reinforcement learning\\nwith a broad scope and in-depth view. We believe our study can provide a\\ncompact guideline for the current advancements in deep reinforcement learning,\\nand help to construct robust deep neural policies with higher generalization\\nskills.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Deep Reinforcement Learning Framework for Dynamic Portfolio Optimization: Evidence from China's Stock Market</td>\n",
       "      <td>Artificial intelligence is transforming financial investment decision-making\\nframeworks, with deep reinforcement learning demonstrating substantial\\npotential in robo-advisory applications. This paper addresses the limitations\\nof traditional portfolio optimization methods in dynamic asset weight\\nadjustment through the development of a deep reinforcement learning-based\\ndynamic optimization model grounded in practical trading processes. The\\nresearch advances two key innovations: first, the introduction of a novel\\nSharpe ratio reward function engineered for Actor-Critic deep reinforcement\\nlearning algorithms, which ensures stable convergence during training while\\nconsistently achieving positive average Sharpe ratios; second, the development\\nof an innovative comprehensive approach to portfolio optimization utilizing\\ndeep reinforcement learning, which significantly enhances model optimization\\ncapability through the integration of random sampling strategies during\\ntraining with image-based deep neural network architectures for\\nmulti-dimensional financial time series data processing, average Sharpe ratio\\nreward functions, and deep reinforcement learning algorithms. The empirical\\nanalysis validates the model using randomly selected constituent stocks from\\nthe CSI 300 Index, benchmarking against established financial econometric\\noptimization models. Backtesting results demonstrate the model's efficacy in\\noptimizing portfolio allocation and mitigating investment risk, yielding\\nsuperior comprehensive performance metrics.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Nesterov's Accelerated quasi-Newton method for Global Routing using Deep Reinforcement Learning</td>\n",
       "      <td>Deep Q-learning method is one of the most popularly used deep reinforcement\\nlearning algorithms which uses deep neural networks to approximate the\\nestimation of the action-value function. Training of the deep Q-network (DQN)\\nis usually restricted to first order gradient based methods. This paper\\nattempts to accelerate the training of deep Q-networks by introducing a second\\norder Nesterov's accelerated quasi-Newton method. We evaluate the performance\\nof the proposed method on deep reinforcement learning using double DQNs for\\nglobal routing. The results show that the proposed method can obtain better\\nrouting solutions compared to the DQNs trained with first order Adam and\\nRMSprop methods.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Brief Survey of Deep Reinforcement Learning</td>\n",
       "      <td>Deep reinforcement learning is poised to revolutionise the field of AI and\\nrepresents a step towards building autonomous systems with a higher level\\nunderstanding of the visual world. Currently, deep learning is enabling\\nreinforcement learning to scale to problems that were previously intractable,\\nsuch as learning to play video games directly from pixels. Deep reinforcement\\nlearning algorithms are also applied to robotics, allowing control policies for\\nrobots to be learned directly from camera inputs in the real world. In this\\nsurvey, we begin with an introduction to the general field of reinforcement\\nlearning, then progress to the main streams of value-based and policy-based\\nmethods. Our survey will cover central algorithms in deep reinforcement\\nlearning, including the deep $Q$-network, trust region policy optimisation, and\\nasynchronous advantage actor-critic. In parallel, we highlight the unique\\nadvantages of deep neural networks, focusing on visual understanding via\\nreinforcement learning. To conclude, we describe several current areas of\\nresearch within the field.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                            title  \\\n",
       "0                                          Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey   \n",
       "1                                                A Survey Analyzing Generalization in Deep Reinforcement Learning   \n",
       "2  A Deep Reinforcement Learning Framework for Dynamic Portfolio Optimization: Evidence from China's Stock Market   \n",
       "3               A Nesterov's Accelerated quasi-Newton method for Global Routing using Deep Reinforcement Learning   \n",
       "4                                                                   A Brief Survey of Deep Reinforcement Learning   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             abstract  \n",
       "0                                                                                                         Deep reinforcement learning augments the reinforcement learning framework and\\nutilizes the powerful representation of deep neural networks. Recent works have\\ndemonstrated the remarkable successes of deep reinforcement learning in various\\ndomains including finance, medicine, healthcare, video games, robotics, and\\ncomputer vision. In this work, we provide a detailed review of recent and\\nstate-of-the-art research advances of deep reinforcement learning in computer\\nvision. We start with comprehending the theories of deep learning,\\nreinforcement learning, and deep reinforcement learning. We then propose a\\ncategorization of deep reinforcement learning methodologies and discuss their\\nadvantages and limitations. In particular, we divide deep reinforcement\\nlearning into seven main categories according to their applications in computer\\nvision, i.e. (i)landmark localization (ii) object detection; (iii) object\\ntracking; (iv) registration on both 2D image and 3D image volumetric data (v)\\nimage segmentation; (vi) videos analysis; and (vii) other applications. Each of\\nthese categories is further analyzed with reinforcement learning techniques,\\nnetwork design, and performance. Moreover, we provide a comprehensive analysis\\nof the existing publicly available datasets and examine source code\\navailability. Finally, we present some open issues and discuss future research\\ndirections on deep reinforcement learning in computer vision  \n",
       "1                                                                                                                                                                                                                                             Reinforcement learning research obtained significant success and attention\\nwith the utilization of deep neural networks to solve problems in high\\ndimensional state or action spaces. While deep reinforcement learning policies\\nare currently being deployed in many different fields from medical applications\\nto large language models, there are still ongoing questions the field is trying\\nto answer on the generalization capabilities of deep reinforcement learning\\npolicies. In this paper, we will formalize and analyze generalization in deep\\nreinforcement learning. We will explain the fundamental reasons why deep\\nreinforcement learning policies encounter overfitting problems that limit their\\ngeneralization capabilities. Furthermore, we will categorize and explain the\\nmanifold solution approaches to increase generalization, and overcome\\noverfitting in deep reinforcement learning policies. From exploration to\\nadversarial analysis and from regularization to robustness our paper provides\\nan analysis on a wide range of subfields within deep reinforcement learning\\nwith a broad scope and in-depth view. We believe our study can provide a\\ncompact guideline for the current advancements in deep reinforcement learning,\\nand help to construct robust deep neural policies with higher generalization\\nskills.  \n",
       "2  Artificial intelligence is transforming financial investment decision-making\\nframeworks, with deep reinforcement learning demonstrating substantial\\npotential in robo-advisory applications. This paper addresses the limitations\\nof traditional portfolio optimization methods in dynamic asset weight\\nadjustment through the development of a deep reinforcement learning-based\\ndynamic optimization model grounded in practical trading processes. The\\nresearch advances two key innovations: first, the introduction of a novel\\nSharpe ratio reward function engineered for Actor-Critic deep reinforcement\\nlearning algorithms, which ensures stable convergence during training while\\nconsistently achieving positive average Sharpe ratios; second, the development\\nof an innovative comprehensive approach to portfolio optimization utilizing\\ndeep reinforcement learning, which significantly enhances model optimization\\ncapability through the integration of random sampling strategies during\\ntraining with image-based deep neural network architectures for\\nmulti-dimensional financial time series data processing, average Sharpe ratio\\nreward functions, and deep reinforcement learning algorithms. The empirical\\nanalysis validates the model using randomly selected constituent stocks from\\nthe CSI 300 Index, benchmarking against established financial econometric\\noptimization models. Backtesting results demonstrate the model's efficacy in\\noptimizing portfolio allocation and mitigating investment risk, yielding\\nsuperior comprehensive performance metrics.  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Deep Q-learning method is one of the most popularly used deep reinforcement\\nlearning algorithms which uses deep neural networks to approximate the\\nestimation of the action-value function. Training of the deep Q-network (DQN)\\nis usually restricted to first order gradient based methods. This paper\\nattempts to accelerate the training of deep Q-networks by introducing a second\\norder Nesterov's accelerated quasi-Newton method. We evaluate the performance\\nof the proposed method on deep reinforcement learning using double DQNs for\\nglobal routing. The results show that the proposed method can obtain better\\nrouting solutions compared to the DQNs trained with first order Adam and\\nRMSprop methods.  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Deep reinforcement learning is poised to revolutionise the field of AI and\\nrepresents a step towards building autonomous systems with a higher level\\nunderstanding of the visual world. Currently, deep learning is enabling\\nreinforcement learning to scale to problems that were previously intractable,\\nsuch as learning to play video games directly from pixels. Deep reinforcement\\nlearning algorithms are also applied to robotics, allowing control policies for\\nrobots to be learned directly from camera inputs in the real world. In this\\nsurvey, we begin with an introduction to the general field of reinforcement\\nlearning, then progress to the main streams of value-based and policy-based\\nmethods. Our survey will cover central algorithms in deep reinforcement\\nlearning, including the deep $Q$-network, trust region policy optimisation, and\\nasynchronous advantage actor-critic. In parallel, we highlight the unique\\nadvantages of deep neural networks, focusing on visual understanding via\\nreinforcement learning. To conclude, we describe several current areas of\\nresearch within the field.  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "943edd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# creating a summarization pipeline\n",
    "pipe =  pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0aceb232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence is transforming financial investment decision-making\n",
      "frameworks, with deep reinforcement learning demonstrating substantial\n",
      "potential in robo-advisory applications. This paper addresses the limitations\n",
      "of traditional portfolio optimization methods in dynamic asset weight\n",
      "adjustment through the development of a deep reinforcement learning-based\n",
      "dynamic optimization model grounded in practical trading processes. The\n",
      "research advances two key innovations: first, the introduction of a novel\n",
      "Sharpe ratio reward function engineered for Actor-Critic deep reinforcement\n",
      "learning algorithms, which ensures stable convergence during training while\n",
      "consistently achieving positive average Sharpe ratios; second, the development\n",
      "of an innovative comprehensive approach to portfolio optimization utilizing\n",
      "deep reinforcement learning, which significantly enhances model optimization\n",
      "capability through the integration of random sampling strategies during\n",
      "training with image-based deep neural network architectures for\n",
      "multi-dimensional financial time series data processing, average Sharpe ratio\n",
      "reward functions, and deep reinforcement learning algorithms. The empirical\n",
      "analysis validates the model using randomly selected constituent stocks from\n",
      "the CSI 300 Index, benchmarking against established financial econometric\n",
      "optimization models. Backtesting results demonstrate the model's efficacy in\n",
      "optimizing portfolio allocation and mitigating investment risk, yielding\n",
      "superior comprehensive performance metrics.\n"
     ]
    }
   ],
   "source": [
    "# load single abstract\n",
    "abstract_1 = df['abstract'][2]\n",
    "print(abstract_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d88a4b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep reinforcement learning has substantial potential in robo-advisory applications. The paper addresses the limitations of traditional portfolio optimization methods. Backtesting results demonstrate the model's efficacy in optimizing portfolio allocation and mitigating investment risk. The research advances two key innovations: the introduction of a novel Sharpe ratio reward function and the development of an innovative comprehensive approach to portfolio optimization.\n"
     ]
    }
   ],
   "source": [
    "# Summarizing the first abstract\n",
    "summary_1 = pipe(abstract_1)\n",
    "print(summary_1[0]['summary_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
